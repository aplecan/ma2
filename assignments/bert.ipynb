{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI & Machine Learning (KAN-CINTO4003U) - Copenhagen Business School | Spring 2025**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part II: BERT\n",
    "\n",
    "Please see the description of the assignment in the README file (section 2) <br>\n",
    "**Guide notebook**: [guides/bert_guide.ipynb](guides/bert_guide.ipynb)\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "<br>\n",
    "\n",
    "* Note that you should report results using a classification report. \n",
    "\n",
    "* Also, remember to include some reflections on your results: how do they compare with the results from Part I, BoW? Are there any hyperparameters that are particularly important?\n",
    "\n",
    "* You should follow the steps given in the `bert_guide` notebook\n",
    "\n",
    "* **Optionally**, you can fine-tune a pre-trained BERT model to classify news articles as is done in [guides/bert_guide_finetuning.ipybb](guides/bert_guide_finetuning.ipybb), the same task as in part 1. As this requires more computational resources, this part is optional. If you do decide to complete this part, you will need to use a GPU (e.g., Google Colab) to train the model. (For reference, training on a 2020 Macbook Pro with 16GB RAM and a M1 chip results in an out-of-memory error). Therefore, we suggest that you use Google Colab or another cloud-based service with a GPU. You can easily upload the `bert_guide_finetuning.ipynb` notebook to Google Colab and run it there.\n",
    "\n",
    "<br>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the project\n",
    "\n",
    "from datasets import load_dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data\n",
    "\n",
    "We can load this data directly from [Hugging Face Datasets](https://huggingface.co/docs/datasets/) - The HuggingFace Hub- into a Pandas DataFrame. Pretty neat!\n",
    "\n",
    "**Note**: This cell will download the dataset and keep it in memory. If you run this cell multiple times, it will download the dataset multiple times.\n",
    "\n",
    "You are welcome to increase the `frac` parameter to load more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 24000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 7600\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load a subset of the training data (20%) and the full test set.\n",
    "ag_news_train = load_dataset(\"fancyzhx/ag_news\", split=\"train[:20%]\")\n",
    "ag_news_test = load_dataset(\"fancyzhx/ag_news\", split=\"test\")\n",
    "\n",
    "# Create a DatasetDict to hold the train and test splits.\n",
    "ag_news = DatasetDict({\n",
    "    \"train\": ag_news_train,\n",
    "    \"test\": ag_news_test\n",
    "})\n",
    "\n",
    "# Print dataset information\n",
    "print(ag_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Use the standard BERT model for feature extraction.\n",
    "embedder = pipeline(\n",
    "    model=\"bert-base-uncased\",      # Standard pre-trained BERT model\n",
    "    tokenizer=\"bert-base-uncased\",  # Corresponding tokenizer\n",
    "    task=\"feature-extraction\",      # Returns embeddings for each token\n",
    "    device=-1                       # Use CPU; change to 0 if you have a GPU\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c86d79f2be54e81bb7f5689393b9d6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'embeddings'],\n",
      "        num_rows: 24000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'embeddings'],\n",
      "        num_rows: 7600\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def get_embeddings(data):\n",
    "    \"\"\"\n",
    "    Extracts the [CLS] embedding for each text entry.\n",
    "    \n",
    "    BERT (and ModernBERT) returns embeddings for each token.\n",
    "    We use the embedding corresponding to the first token ([CLS])\n",
    "    as a representation for the whole text.\n",
    "    \"\"\"\n",
    "    # Process a batch of texts to obtain token embeddings.\n",
    "    embeddings = embedder(data[\"text\"])\n",
    "    # Extract the first token's embedding ([CLS]) for each example.\n",
    "    cls_embeddings = [e[0][0] for e in embeddings]\n",
    "    return {\"embeddings\": cls_embeddings}\n",
    "\n",
    "# Map the get_embeddings function over the dataset (batched processing for speed).\n",
    "# Adjust batch_size if you run into memory issues.\n",
    "ag_news = ag_news.map(get_embeddings, batched=True, batch_size=8)\n",
    "\n",
    "# Inspect the updated dataset structure.\n",
    "print(ag_news)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (24000, 768), y_train shape: (24000,)\n",
      "X_test shape: (7600, 768), y_test shape: (7600,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert the embeddings and labels from the dataset into NumPy arrays.\n",
    "X_train = np.array(ag_news[\"train\"][\"embeddings\"])  # BERT embeddings as features\n",
    "y_train = np.array(ag_news[\"train\"][\"label\"])         # Labels\n",
    "\n",
    "X_test = np.array(ag_news[\"test\"][\"embeddings\"])\n",
    "y_test = np.array(ag_news[\"test\"][\"label\"])\n",
    "\n",
    "# Check the shapes of the resulting arrays.\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on the Training Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.93      6195\n",
      "           1       0.97      0.99      0.98      5856\n",
      "           2       0.88      0.87      0.88      5601\n",
      "           3       0.90      0.91      0.90      6348\n",
      "\n",
      "    accuracy                           0.92     24000\n",
      "   macro avg       0.92      0.92      0.92     24000\n",
      "weighted avg       0.92      0.92      0.92     24000\n",
      "\n",
      "Performance on the Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89      1900\n",
      "           1       0.95      0.95      0.95      1900\n",
      "           2       0.83      0.82      0.83      1900\n",
      "           3       0.83      0.86      0.84      1900\n",
      "\n",
      "    accuracy                           0.88      7600\n",
      "   macro avg       0.88      0.88      0.88      7600\n",
      "weighted avg       0.88      0.88      0.88      7600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Train a baseline Logistic Regression classifier on the BERT embeddings.\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the training set and evaluate performance.\n",
    "y_pred_train = lr.predict(X_train)\n",
    "print(\"Performance on the Training Set:\")\n",
    "print(classification_report(y_train, y_pred_train))\n",
    "\n",
    "# Predict on the test set and evaluate performance.\n",
    "y_pred = lr.predict(X_test)\n",
    "print(\"Performance on the Test Set:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Performance with Logistic Regression (C=0.5):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.88      0.89      1900\n",
      "           1       0.96      0.96      0.96      1900\n",
      "           2       0.83      0.82      0.83      1900\n",
      "           3       0.83      0.87      0.85      1900\n",
      "\n",
      "    accuracy                           0.88      7600\n",
      "   macro avg       0.88      0.88      0.88      7600\n",
      "weighted avg       0.88      0.88      0.88      7600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Experiment with a different regularization parameter (C).\n",
    "# A lower C value applies stronger regularization.\n",
    "lr_exp = LogisticRegression(max_iter=1000, C=0.5)\n",
    "lr_exp.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the experimental model on the test set.\n",
    "y_pred_exp = lr_exp.predict(X_test)\n",
    "print(\"Test Set Performance with Logistic Regression (C=0.5):\")\n",
    "print(classification_report(y_test, y_pred_exp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection on BERT Embeddings and Classifier Performance\n",
    "\n",
    "In this experiment, I used a pre-trained BERT model (via the Hugging Face pipeline) to embed the AG News articles. The resulting embeddings are 768-dimensional vectors, and the processed dataset contains 24,000 training examples and 7,600 test examples.\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "- Feature representation: the BERT embeddings provide a dense, semantic representation of the text. Each article is converted into a fixed-length 768-dimensional vector, capturing nuanced contextual information that is hard to achieve with traditional methods such as BoW.\n",
    "\n",
    "- Dataset and embedding extraction: After embedding, the training set has a shape of (24,000, 768) and the test set (7,600, 768). This consistency in dimensions makes the embeddings directly usable by standard classifiers like Logistic Regression.\n",
    "\n",
    "- Classifier performance on training data: The Logistic Regression classifier achieved an accuracy of 92% on the training set with high precision and recall across all categories:\n",
    "  - Class 0 (World): Precision = 0.93, Recall = 0.92, F1 = 0.93\n",
    "  - Class 1 (Sports): Precision = 0.97, Recall = 0.99, F1 = 0.98\n",
    "  - Class 2 (Business): Precision = 0.88, Recall = 0.87, F1 = 0.88\n",
    "  - Class 3 (Sci/Tech): Precision = 0.90, Recall = 0.91, F1 = 0.90\n",
    "\n",
    "- Generalization to test data: On the test set, the classifier achieves an overall accuracy of 88%. The per-class performance is slightly lower than the training metrics, indicating that the model generalizes reasonably well:\n",
    "  - Class 0: F1 ≈ 0.89\n",
    "  - Class 1: F1 ≈ 0.95\n",
    "  - Class 2: F1 ≈ 0.83\n",
    "  - Class 3: F1 ≈ 0.84\n",
    "  \n",
    "  This moderate drop in performance from training to test data suggests that the BERT embeddings help reduce overfitting compared to more sparse representations.\n",
    "\n",
    "- Hyperparameter tuning:\n",
    "  Experimenting with a lower regularization parameter (C=0.5) resulted in nearly identical performance on the test set. This suggests that the default regularization of the Logistic Regression model is already well-suited for the robust features provided by BERT, and minor adjustments do not significantly impact performance.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "OVerall, the use of BERT embeddings has proven effective for the AG News classification task:\n",
    "- Good Feature Extraction, the 768-dimensional embeddings capture rich semantic content, which in turn enables the classifier to achieve high accuracy.\n",
    "- Good Performance, with a training accuracy of 92% and test accuracy of 88%, the gap between training and test performance is moderate, indicating good generalization.\n",
    "- Hyperparameter Sensitivity, changes to the regularization parameter did not dramatically alter performance, which implies that the embeddings provide a stable basis for classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml25-ma1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
