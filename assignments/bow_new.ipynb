{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI & Machine Learning (KAN-CINTO4003U) - Copenhagen Business School | Spring 2025**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part I: Bag-of-Words Model\n",
    "\n",
    "Please see the description of the assignment in the README file (section 1) <br>\n",
    "**Guide notebook**: [guides/bow_guide.ipynb](guides/bow_guide.ipynb)\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "<br>\n",
    "\n",
    "* Note that you should report results using a classification report. \n",
    "\n",
    "* Also, remember to include some reflections on your results: Are there any hyperparameters that are particularly important?\n",
    "\n",
    "* You should follow the steps given in the `bow_guide` notebook\n",
    "\n",
    "<br>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the project\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data\n",
    "\n",
    "We can load this data directly from [Hugging Face Datasets](https://huggingface.co/docs/datasets/) - The HuggingFace Hub- into a Pandas DataFrame. Pretty neat!\n",
    "\n",
    "**Note**: This cell will download the dataset and keep it in memory. If you run this cell multiple times, it will download the dataset multiple times.\n",
    "\n",
    "You are welcome to increase the `frac` parameter to load more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shapes: (120000, 2) (7600, 2)\n"
     ]
    }
   ],
   "source": [
    "# Define file paths for the training and test datasets.\n",
    "splits = {\n",
    "    'train': 'data/train-00000-of-00001.parquet',\n",
    "    'test': 'data/test-00000-of-00001.parquet'\n",
    "}\n",
    "\n",
    "# Load the AG News dataset using pandas.read_parquet.\n",
    "# This downloads the data from the Hugging Face Hub.\n",
    "train = pd.read_parquet(\"hf://datasets/fancyzhx/ag_news/\" + splits[\"train\"])\n",
    "test = pd.read_parquet(\"hf://datasets/fancyzhx/ag_news/\" + splits[\"test\"])\n",
    "\n",
    "print(\"Original shapes:\", train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed shapes: (1200, 2) (760, 2)\n"
     ]
    }
   ],
   "source": [
    "# Define a mapping from numerical labels to their string categories.\n",
    "label_map = {\n",
    "    0: 'World',\n",
    "    1: 'Sports',\n",
    "    2: 'Business',\n",
    "    3: 'Sci/Tech'\n",
    "}\n",
    "\n",
    "def preprocess(df: pd.DataFrame, frac: float = 1e-2, label_map: dict[int, str] = label_map, seed: int = 42) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess the dataset by:\n",
    "    - Mapping numeric labels to category names.\n",
    "    - Filtering to only include rows with valid labels.\n",
    "    - Sampling a fraction of the data for quick experimentation (stratified by label).\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df\n",
    "        .assign(label=lambda x: x['label'].map(label_map))\n",
    "        [lambda df: df['label'].isin(label_map.values())]\n",
    "        .groupby('label')[[\"text\", \"label\"]]\n",
    "        .apply(lambda x: x.sample(frac=frac, random_state=seed))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "# Apply preprocessing to both training and test sets.\n",
    "train_df = preprocess(train, frac=0.01)\n",
    "test_df = preprocess(test, frac=0.1)\n",
    "\n",
    "# Free memory by deleting the original dataframes.\n",
    "del train\n",
    "del test\n",
    "\n",
    "print(\"Preprocessed shapes:\", train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (960,)\n",
      "Validation set shape: (240,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the training data into training and validation sets.\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_df[\"text\"], train_df[\"label\"],\n",
    "    test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Validation set shape:\", X_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW matrix shape (training): (960, 7634)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer to transform text data into a Bag-of-Words representation.\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the training text and transform it.\n",
    "X_train_vectorized = cv.fit_transform(X_train)\n",
    "print(\"BoW matrix shape (training):\", X_train_vectorized.shape)\n",
    "\n",
    "# Transform the validation and test text using the fitted vectorizer.\n",
    "X_val_vectorized = cv.transform(X_val)\n",
    "X_test_vectorized = cv.transform(test_df[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on the Training Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       World       1.00      1.00      1.00       238\n",
      "      Sports       1.00      1.00      1.00       240\n",
      "    Business       1.00      1.00      1.00       240\n",
      "    Sci/Tech       1.00      1.00      1.00       242\n",
      "\n",
      "    accuracy                           1.00       960\n",
      "   macro avg       1.00      1.00      1.00       960\n",
      "weighted avg       1.00      1.00      1.00       960\n",
      "\n",
      "Performance on the Validation Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       World       0.76      0.68      0.72        62\n",
      "      Sports       0.69      0.60      0.64        60\n",
      "    Business       0.79      0.87      0.83        60\n",
      "    Sci/Tech       0.78      0.90      0.83        58\n",
      "\n",
      "    accuracy                           0.76       240\n",
      "   macro avg       0.75      0.76      0.75       240\n",
      "weighted avg       0.75      0.76      0.75       240\n",
      "\n",
      "Performance on the Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       World       0.74      0.72      0.73       190\n",
      "      Sports       0.75      0.72      0.73       190\n",
      "    Business       0.83      0.88      0.86       190\n",
      "    Sci/Tech       0.79      0.79      0.79       190\n",
      "\n",
      "    accuracy                           0.78       760\n",
      "   macro avg       0.78      0.78      0.78       760\n",
      "weighted avg       0.78      0.78      0.78       760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Train a Logistic Regression classifier on the BoW representation.\n",
    "# Increase max_iter to ensure convergence.\n",
    "lr_clf = LogisticRegression(max_iter=1000)\n",
    "lr_clf.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Predict on training and validation sets.\n",
    "y_train_pred = lr_clf.predict(X_train_vectorized)\n",
    "y_val_pred = lr_clf.predict(X_val_vectorized)\n",
    "\n",
    "# Evaluate classifier performance.\n",
    "print(\"Performance on the Training Set:\")\n",
    "print(classification_report(y_train, y_train_pred, target_names=label_map.values()))\n",
    "\n",
    "print(\"Performance on the Validation Set:\")\n",
    "print(classification_report(y_val, y_val_pred, target_names=label_map.values()))\n",
    "\n",
    "# Evaluate performance on the test set.\n",
    "y_test_pred = lr_clf.predict(X_test_vectorized)\n",
    "print(\"Performance on the Test Set:\")\n",
    "print(classification_report(test_df[\"label\"], y_test_pred, target_names=label_map.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Performance with Logistic Regression (C=0.5):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       World       0.76      0.68      0.72        62\n",
      "      Sports       0.69      0.60      0.64        60\n",
      "    Business       0.79      0.87      0.83        60\n",
      "    Sci/Tech       0.78      0.90      0.83        58\n",
      "\n",
      "    accuracy                           0.76       240\n",
      "   macro avg       0.75      0.76      0.75       240\n",
      "weighted avg       0.75      0.76      0.75       240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Experiment with a different hyperparameter: regularization strength (C).\n",
    "# A lower C value means stronger regularization.\n",
    "lr_clf_exp = LogisticRegression(max_iter=1000, C=0.5)\n",
    "lr_clf_exp.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Predict on the validation set with the new model.\n",
    "y_val_pred_exp = lr_clf_exp.predict(X_val_vectorized)\n",
    "\n",
    "print(\"Validation Performance with Logistic Regression (C=0.5):\")\n",
    "print(classification_report(y_val, y_val_pred_exp, target_names=label_map.values()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations and Analysis\n",
    "\n",
    "- Dataset Size and Preprocessing:\n",
    "  - Original Data: The initial training set had 120,000 rows and the test set 7,600 rows.\n",
    "  - After Preprocessing: Sampling reduced the datasets to 1,200 rows for training and 760 rows for testing. Further splitting the training set yielded 960 samples for training and 240 for validation.\n",
    "\n",
    "- Feature Representation:\n",
    "  - The BoW vectorization produced a document-term matrix of shape (960, 7634), meaning that 7,634 unique tokens were extracted from the training data.\n",
    "\n",
    "- Training Performance:\n",
    "  - The classifier achieved perfect scores (100% precision, recall, and F1-score) on the training set. This suggests that the model is capable of memorizing the training data—possibly due to the small dataset size—but also indicates a risk of overfitting.\n",
    "\n",
    "- Validation and Test Performance:\n",
    "  - On the validation set, overall accuracy dropped to 76%, with performance varying by category:\n",
    "    - World: F1-score of 0.72\n",
    "    - Sports: F1-score of 0.64\n",
    "    - Business: F1-score of 0.83\n",
    "    - Sci/Tech: F1-score of 0.83\n",
    "  - The test set results are consistent with these findings, achieving an overall accuracy of 78%.\n",
    "\n",
    "- Hyperparameter Experimentation:\n",
    "  - Adjusting the regularization strength (using Logistic Regression with C=0.5) yielded the same performance on the validation set as the baseline model. This indicates that merely changing the regularization parameter, at least with the current setup and sampled data, did not have a significant impact on model generalization.\n",
    "\n",
    "COnsidering the results of both iterations, while the model fits the training data with a score of 1, the drop in performance on the validation and test sets suggests that further tuning—perhaps through additional hyperparameter adjustments, alternative classifiers, or enhanced feature engineering—is necessary to improve generalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml25-ma1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
